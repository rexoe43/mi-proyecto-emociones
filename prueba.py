import cv2
from deepface import DeepFace
import customtkinter as ctk
import tkinter as tk
import random
from datetime import datetime
import numpy as np
from collections import Counter
import time

# Diccionario de emociones con preguntas
preguntas_emociones = {
    'happy': [
        "Â¿QuÃ© te hizo sentir feliz hoy?",
        "Â¿Con quiÃ©n compartiste ese momento especial?",
        "Â¿Quieres guardar este recuerdo?",
        "Â¿QuÃ© fue lo mÃ¡s divertido del dÃ­a?",
    ],
    'sad': [
        "Â¿QuÃ© te hizo sentir triste?",
        "Â¿Hay algo que puedas hacer para mejorar tu dÃ­a?",
        "Â¿Te gustarÃ­a hablar con alguien sobre eso?",
        "Â¿QuÃ© podrÃ­as hacer para sentirte un poco mejor?",
    ],
    'angry': [
        "Â¿QuÃ© provocÃ³ tu enojo?",
        "Â¿Lo pudiste expresar de forma saludable?",
        "Â¿QuÃ© te ayudarÃ­a a calmarte ahora?",
        "Â¿Vale la pena seguir enojado?",
    ],
    'surprise': [
        "Â¿QuÃ© te sorprendiÃ³?",
        "Â¿Fue una sorpresa positiva o negativa?",
        "Â¿CÃ³mo reaccionaste ante eso?",
        "Â¿Te gustarÃ­a compartirlo con alguien?",
    ],
    'fear': [
        "Â¿QuÃ© te causÃ³ miedo?",
        "Â¿Era un miedo justificado?",
        "Â¿CÃ³mo puedes enfrentar esa situaciÃ³n?",
        "Â¿Te sientes mÃ¡s seguro ahora?",
    ],
    'disgust': [
        "Â¿QuÃ© te causÃ³ esta sensaciÃ³n?",
        "Â¿Fue algo que viste o experimentaste?",
        "Â¿CÃ³mo puedes evitar esa situaciÃ³n?",
        "Â¿Te sientes mejor ahora?",
    ],
    'neutral': [
        "Â¿CÃ³mo te sientes realmente en este momento?",
        "Â¿Hay algo que te gustarÃ­a cambiar de tu dÃ­a?",
        "Â¿QuÃ© podrÃ­as hacer para sentirte mÃ¡s motivado?",
        "Â¿Te gustarÃ­a reflexionar sobre algo?",
    ]
}

class DetectorEmociones:
    def __init__(self):
        self.emotion_history = []
        self.confidence_threshold = 60  # Umbral de confianza mÃ­nimo
        self.stabilization_frames = 10  # Frames para estabilizar la emociÃ³n
        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        
    def preprocess_frame(self, frame):
        """Preprocesa el frame para mejorar la detecciÃ³n"""
        # Mejora el contraste y brillo
        lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
        l = clahe.apply(l)
        enhanced = cv2.merge([l, a, b])
        enhanced = cv2.cvtColor(enhanced, cv2.COLOR_LAB2BGR)
        return enhanced
    
    def detect_face_regions(self, frame):
        """Detecta mÃºltiples regiones faciales para mayor precisiÃ³n"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = self.face_cascade.detectMultiScale(
            gray, 
            scaleFactor=1.1, 
            minNeighbors=5, 
            minSize=(60, 60),
            flags=cv2.CASCADE_SCALE_IMAGE
        )
        return faces
    
    def analyze_emotion_multiple_methods(self, frame):
        """Analiza emociones usando mÃºltiples mÃ©todos para mayor precisiÃ³n"""
        results = []
        
        # MÃ©todo 1: AnÃ¡lisis del frame completo
        try:
            result = DeepFace.analyze(
                frame, 
                actions=['emotion'], 
                enforce_detection=False,
                detector_backend='opencv'
            )
            if isinstance(result, list):
                result = result[0]
            results.append(result)
        except:
            pass
        
        # MÃ©todo 2: AnÃ¡lisis de regiones faciales detectadas
        faces = self.detect_face_regions(frame)
        for (x, y, w, h) in faces:
            if w > 80 and h > 80:  # Solo rostros de tamaÃ±o razonable
                face_roi = frame[y:y+h, x:x+w]
                try:
                    result = DeepFace.analyze(
                        face_roi, 
                        actions=['emotion'], 
                        enforce_detection=False,
                        detector_backend='opencv'
                    )
                    if isinstance(result, list):
                        result = result[0]
                    # Ajustar las coordenadas de la regiÃ³n
                    result['region'] = {'x': x, 'y': y, 'w': w, 'h': h}
                    results.append(result)
                except:
                    pass
        
        return results
    
    def get_most_confident_emotion(self, results):
        """Obtiene la emociÃ³n con mayor confianza de mÃºltiples anÃ¡lisis"""
        if not results:
            return None
        
        best_result = None
        best_confidence = 0
        
        for result in results:
            dominant_emotion = result['dominant_emotion']
            confidence = result['emotion'][dominant_emotion]
            
            if confidence > best_confidence and confidence > self.confidence_threshold:
                best_confidence = confidence
                best_result = result
        
        return best_result
    
    def stabilize_emotion(self, emotion, confidence):
        """Estabiliza la emociÃ³n detectada usando un historial"""
        if confidence > self.confidence_threshold:
            self.emotion_history.append(emotion)
        
        # Mantener solo los Ãºltimos N frames
        if len(self.emotion_history) > self.stabilization_frames:
            self.emotion_history = self.emotion_history[-self.stabilization_frames:]
        
        # Si tenemos suficientes muestras, usar la mÃ¡s frecuente
        if len(self.emotion_history) >= 5:
            emotion_counts = Counter(self.emotion_history)
            most_common = emotion_counts.most_common(1)[0]
            if most_common[1] >= 3:  # Al menos 3 detecciones iguales
                return most_common[0]
        
        return emotion if confidence > self.confidence_threshold else None

def detectar_emocion_en_camara():
    detector = DetectorEmociones()
    cap = cv2.VideoCapture(0)
    
    # Configurar la cÃ¡mara para mejor calidad
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    cap.set(cv2.CAP_PROP_FPS, 30)
    
    emocion_detectada = None
    confianza = 0
    frames_sin_deteccion = 0
    max_frames_sin_deteccion = 30
    
    print("[INFO] CÃ¡mara activada. AsegÃºrate de tener buena iluminaciÃ³n.")
    print("[INFO] MantÃ©n el rostro centrado y mira a la cÃ¡mara.")
    print("[INFO] Presiona 'q' para continuar al cuestionario...")
    print("[INFO] Presiona 'r' para reiniciar el historial de emociones...")
    
    while True:
        ret, frame = cap.read()
        if not ret:
            continue
        
        # Voltear el frame horizontalmente para efecto espejo
        frame = cv2.flip(frame, 1)
        
        # Preprocesar el frame
        enhanced_frame = detector.preprocess_frame(frame)
        
        try:
            # Analizar emociones con mÃºltiples mÃ©todos
            results = detector.analyze_emotion_multiple_methods(enhanced_frame)
            best_result = detector.get_most_confident_emotion(results)
            
            if best_result:
                current_emotion = best_result['dominant_emotion']
                current_confidence = best_result['emotion'][current_emotion]
                
                # Estabilizar la emociÃ³n
                stable_emotion = detector.stabilize_emotion(current_emotion, current_confidence)
                
                if stable_emotion:
                    emocion_detectada = stable_emotion
                    confianza = current_confidence
                    frames_sin_deteccion = 0
                
                # Obtener regiÃ³n facial
                if 'region' in best_result:
                    area = best_result['region']
                    x, y, w, h = area['x'], area['y'], area['w'], area['h']
                else:
                    # Si no hay regiÃ³n especÃ­fica, usar detecciÃ³n de rostro
                    faces = detector.detect_face_regions(frame)
                    if len(faces) > 0:
                        x, y, w, h = faces[0]
                    else:
                        x, y, w, h = 0, 0, 0, 0
                
                # Dibujar cuadro y informaciÃ³n
                if w > 0 and h > 0:
                    # Cuadro principal
                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
                    
                    # Texto con emociÃ³n y confianza
                    texto_principal = f"{current_emotion.upper()} ({current_confidence:.1f}%)"
                    cv2.putText(frame, texto_principal, (x, y - 10), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
                    
                    # Texto con emociÃ³n estabilizada
                    if stable_emotion:
                        texto_estable = f"Estable: {stable_emotion.upper()}"
                        cv2.putText(frame, texto_estable, (x, y + h + 25), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
                
                # Mostrar historial de emociones
                if len(detector.emotion_history) > 0:
                    historial_texto = f"Historial: {len(detector.emotion_history)} muestras"
                    cv2.putText(frame, historial_texto, (10, frame.shape[0] - 30), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
            
            else:
                frames_sin_deteccion += 1
                if frames_sin_deteccion < max_frames_sin_deteccion:
                    cv2.putText(frame, "Detectando rostro...", (30, 40),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
                else:
                    cv2.putText(frame, "No se detecta rostro - Mejora la iluminacion", (30, 40),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        except Exception as e:
            frames_sin_deteccion += 1
            error_msg = f"Error en deteccion: {str(e)[:30]}"
            cv2.putText(frame, error_msg, (30, 40),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
        
        # Mostrar estado actual
        if emocion_detectada:
            status_text = f"Emocion final: {emocion_detectada.upper()} - Presiona 'q' para continuar"
            cv2.putText(frame, status_text, (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        
        # Mostrar controles
        cv2.putText(frame, "Controles: 'q' = continuar, 'r' = reiniciar", (10, frame.shape[0] - 10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        # Mostrar video
        cv2.imshow("Detector de Emociones Mejorado", frame)
        
        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'):
            break
        elif key == ord('r'):
            detector.emotion_history = []
            emocion_detectada = None
            print("[INFO] Historial de emociones reiniciado")
    
    cap.release()
    cv2.destroyAllWindows()
    return emocion_detectada

def mostrar_gui(emocion_detectada):
    if emocion_detectada not in preguntas_emociones:
        emocion_detectada = 'neutral'
    
    pregunta = random.choice(preguntas_emociones[emocion_detectada])
    
    ctk.set_appearance_mode("dark")
    ctk.set_default_color_theme("blue")
    
    app = ctk.CTk()
    app.geometry("1000x700")
    app.title("Detector Emocional - Diario Personal")
    
    # TÃ­tulo principal
    title_label = ctk.CTkLabel(
        app, 
        text=f"EMOCIÃ“N DETECTADA: {emocion_detectada.upper()}", 
        font=("Arial", 28, "bold"),
        text_color="#00ff88"
    )
    title_label.pack(pady=20)
    
    # DescripciÃ³n de la emociÃ³n
    descripciones = {
        'happy': "ðŸ˜Š Te ves feliz y radiante",
        'sad': "ðŸ˜¢ Pareces un poco triste",
        'angry': "ðŸ˜  DetectÃ© algo de enojo",
        'surprise': "ðŸ˜² Te ves sorprendido",
        'fear': "ðŸ˜¨ Pareces preocupado",
        'disgust': "ðŸ¤¢ Algo te disgusta",
        'neutral': "ðŸ˜ Te ves tranquilo"
    }
    
    desc_label = ctk.CTkLabel(
        app, 
        text=descripciones.get(emocion_detectada, "Estado emocional detectado"),
        font=("Arial", 18),
        text_color="#cccccc"
    )
    desc_label.pack(pady=10)
    
    # Pregunta personalizada
    pregunta_label = ctk.CTkLabel(
        app, 
        text=pregunta, 
        font=("Arial", 22), 
        wraplength=900,
        text_color="#ffffff"
    )
    pregunta_label.pack(pady=40)
    
    # Ãrea de texto para respuesta mÃ¡s grande
    respuesta_text = ctk.CTkTextbox(
        app, 
        height=150,
        width=800,
        font=("Arial", 16)
    )
    respuesta_text.pack(pady=20)
    
    # FunciÃ³n para guardar respuesta
    def guardar_respuesta():
        respuesta = respuesta_text.get("1.0", tk.END).strip()
        if respuesta != "":
            with open("diario_emocional.txt", "a", encoding="utf-8") as f:
                timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                f.write(f"\n{'='*50}\n")
                f.write(f"FECHA: {timestamp}\n")
                f.write(f"EMOCIÃ“N: {emocion_detectada.upper()}\n")
                f.write(f"PREGUNTA: {pregunta}\n")
                f.write(f"RESPUESTA: {respuesta}\n")
                f.write(f"{'='*50}\n")
            
            respuesta_text.delete("1.0", tk.END)
            pregunta_label.configure(text="Â¡Respuesta guardada en tu diario emocional! ðŸ“")
            guardar_btn.configure(text="Respuesta Guardada âœ“", state="disabled")
            
            # Reactivar el botÃ³n despuÃ©s de 2 segundos
            app.after(2000, lambda: [
                guardar_btn.configure(text="Guardar otra respuesta", state="normal"),
                pregunta_label.configure(text=random.choice(preguntas_emociones[emocion_detectada]))
            ])
    
    # FunciÃ³n para nueva pregunta
    def nueva_pregunta():
        nueva_preg = random.choice(preguntas_emociones[emocion_detectada])
        pregunta_label.configure(text=nueva_preg)
    
    # Botones
    button_frame = ctk.CTkFrame(app, fg_color="transparent")
    button_frame.pack(pady=20)
    
    guardar_btn = ctk.CTkButton(
        button_frame, 
        text="Guardar respuesta", 
        command=guardar_respuesta,
        font=("Arial", 18),
        height=50,
        width=200
    )
    guardar_btn.pack(side="left", padx=10)
    
    nueva_pregunta_btn = ctk.CTkButton(
        button_frame, 
        text="Nueva pregunta", 
        command=nueva_pregunta,
        font=("Arial", 18),
        height=50,
        width=200,
        fg_color="#ff6b6b"
    )
    nueva_pregunta_btn.pack(side="left", padx=10)
    
    # InformaciÃ³n adicional
    info_label = ctk.CTkLabel(
        app, 
        text="ðŸ’¡ Tip: TÃ³mate tu tiempo para reflexionar. Tus pensamientos son importantes.",
        font=("Arial", 14),
        text_color="#888888"
    )
    info_label.pack(pady=20)
    
    app.mainloop()

# Ejecutar flujo completo
if __name__ == "__main__":
    print("ðŸŽ­ DETECTOR DE EMOCIONES MEJORADO ðŸŽ­")
    print("=====================================")
    print("Iniciando detecciÃ³n de emociones...")
    
    emocion = detectar_emocion_en_camara()
    
    if emocion:
        print(f"\nâœ… EmociÃ³n detectada: {emocion.upper()}")
        print("Abriendo diario emocional...")
        mostrar_gui(emocion)
    else:
        print("\nâŒ No se detectÃ³ una emociÃ³n vÃ¡lida.")
        print("Consejos para mejorar la detecciÃ³n:")
        print("- AsegÃºrate de tener buena iluminaciÃ³n")
        print("- MantÃ©n el rostro centrado en la cÃ¡mara")
        print("- Evita movimientos bruscos")
        print("- Prueba con diferentes expresiones")
        print("\nVuelve a intentarlo.")